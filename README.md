## Hi, I'm **[Hua-zhang Hu](https://scholar.google.com/citations?user=wlJOdBQAAAAJ&hl=zh-CN)** ðŸ‘‹

ðŸ˜„ Iâ€™m a Second-Year Master of [SVIP-Lab](https://svip-lab.github.io/team.html) in [ShanghaiTech University](https://www.shanghaitech.edu.cn/), supervised by [Shenghua Gao](https://scholar.google.com/citations?hl=zh-CN&user=fe-1v0MAAAAJ)

ðŸ”­ **I mainly focus on:**
 * Video Understanding and Activity Analysis
 * Multi-modality Representation Learning 

ðŸŒ± **Publications:**
 * **Weakly Supervised Video Representation Learning with Unaligned Text for Sequential Videos** `CVPR 2023`.  
 [Paper](https://arxiv.org/pdf/2303.12370.pdf) | [Code](https://github.com/svip-lab/WeakSVR) 
 * **TransRAC: Encoding Multi-scale Temporal Correlation with Transformers for Repetitive Action Counting** `CVPR 2022`.    
[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Hu_TransRAC_Encoding_Multi-Scale_Temporal_Correlation_With_Transformers_for_Repetitive_Action_CVPR_2022_paper.html) | [Code](https://github.com/SvipRepetitionCounting/TransRAC) | [Oral Presentation](https://www.youtube.com/watch?v=SFpUS9mHHpk)

ðŸ’¬ **News:**
- 2023-03: A paper about video representation learning is accepted on [`CVPR 2023`](https://cvpr.thecvf.com/).
- 2022-06: We are invited to oral presentation with virtual attendance on `CVPR 2022`.
- 2022-03: A paper about repetitive action counting is accepted for an **Oral presentation**  on [CVPR 2022](https://cvpr2022.thecvf.com/).

ðŸ“« Welcome to communicate with me. Reach me through: huhzh@qq.com  
 
<!-- 

[![Ambition's GitHub stats](https://github-readme-stats.vercel.app/api?username=957001934&show_icons=true)](https://github.com/anuraghazra/github-readme-stats)

 -->
